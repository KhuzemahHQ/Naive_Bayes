{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes for Text Classification\n",
    "\n",
    "### Introduction\n",
    "\n",
    "In this notebook, you will be implementing a Naive Bayes model to classify sentences based off their emotions.\n",
    "\n",
    "The Naive Bayes model is a probabilistic model that uses Bayes' Theorem to calculate the probability of a label given some observed features. In this case, we will be using the Naive Bayes model to calculate the probability of a sentence belonging to a certain emotion given the words in the sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import all required libraries here\n",
    "\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "import math\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, accuracy_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and Preprocessing the Dataset\n",
    "\n",
    "We will be working with the [dair-ai/emotion](https://huggingface.co/datasets/dair-ai/emotion) dataset. This contains 6 classes of emotions: `joy`, `sadness`, `anger`, `fear`, `love`, and `surprise`.\n",
    "\n",
    "Instead of downloading the dataset manually, we will be using the [`datasets`](https://huggingface.co/docs/datasets) library to download the dataset for us. This is a library in the HuggingFace ecosystem that allows us to easily download and use datasets for NLP tasks. Outside of just downloading the dataset, it also provides a standard interface for accessing the data, which makes it easy to use with other libraries like Pandas and PyTorch. You can take a look at the huge list of datasets available [here](https://huggingface.co/datasets).\n",
    "\n",
    "In the following cells,\n",
    "\n",
    "1. Load in the dataset (It should already be split into train, validation, and test sets.)\n",
    "\n",
    "2. Define a dictionary mapping the emotion labels to integers. You can find these on the dataset page linked above.\n",
    "\n",
    "3. Format each split of the dataset into a Pandas DataFrame. The columns should be `text` and `label`, where `text` is the sentence and `label` is the emotion label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\khuze\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "No config specified, defaulting to: emotion/split\n",
      "Found cached dataset emotion (C:/Users/khuze/.cache/huggingface/datasets/dair-ai___emotion/split/1.0.0/cca5efe2dfeb58c1d098e0f9eeb200e9927d889b5a03c67097275dfb5fe463bd)\n",
      "100%|██████████| 3/3 [00:00<00:00, 597.11it/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"dair-ai/emotion\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sadness': 0, 'joy': 1, 'love': 2, 'anger': 3, 'fear': 4, 'surprise': 5}\n"
     ]
    }
   ],
   "source": [
    "emotion_list = [\"sadness\", \"joy\", \"love\", \"anger\", \"fear\", \"surprise\"]\n",
    "emo_dict = dict()\n",
    "for i in range(len(emotion_list)):\n",
    "    emo_dict[emotion_list[i]] = i\n",
    "\n",
    "print(emo_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   text  label\n",
      "0     im feeling quite sad and sorry for myself but ...      0\n",
      "1     i feel like i am still looking at a blank canv...      0\n",
      "2                        i feel like a faithful servant      2\n",
      "3                     i am just feeling cranky and blue      3\n",
      "4     i can have for a treat or if i am feeling festive      1\n",
      "...                                                 ...    ...\n",
      "1995  im having ssa examination tomorrow in the morn...      0\n",
      "1996  i constantly worry about their fight against n...      1\n",
      "1997  i feel its important to share this info for th...      1\n",
      "1998  i truly feel that if you are passionate enough...      1\n",
      "1999  i feel like i just wanna buy any cute make up ...      1\n",
      "\n",
      "[2000 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Available splits: ['test', 'train', 'validation']\n",
    "\n",
    "def convert_dataset_to_pd(data):\n",
    "    new_format = pd.DataFrame.from_dict(data)\n",
    "    return new_format\n",
    "\n",
    "train_data = convert_dataset_to_pd(dataset['train'])\n",
    "validation_data = convert_dataset_to_pd(dataset['validation'])\n",
    "test_data = convert_dataset_to_pd(dataset['test'])\n",
    "\n",
    "print(validation_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've gotten a feel for the dataset, we might want to do some cleaning or preprocessing before continuing. For example, we might want to remove punctuation and other alphanumeric characters, lowercase all the text, strip away extra whitespace, and remove stopwords.\n",
    "\n",
    "In the cell below, there is a function that does exactly the following described above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def punc_rem(text):\n",
    "    text = text.lower()\n",
    "    text  = \"\".join([char for char in text if char not in string.punctuation])\n",
    "    text = re.sub('[0-9]+', '', text)\n",
    "    return text\n",
    "\n",
    "def tokenize(text):\n",
    "    text = re.split('\\W+', text)\n",
    "    return text\n",
    "\n",
    "stopword = ['ourselves', 'hers', 'between', 'yourself', 'but', 'again', 'there', 'about', 'once', \n",
    "             'during', 'out', 'very', 'having', 'with', 'they', 'own', 'an', 'be', 'some','for', 'do', 'its', 'yours', \n",
    "             'such', 'into', 'of', 'most', 'itself', 'other', 'off', 'is', 's', 'am', 'or', 'who', 'as', 'from', 'him', \n",
    "             'each', 'the', 'themselves', 'until', 'below', 'are', 'we', 'these', 'your', 'his', 'through', 'don', 'nor', \n",
    "             'me', 'were', 'her', 'more', 'himself', 'this', 'down', 'should', 'our', 'their', 'while', 'above', 'both', 'up', \n",
    "             'to', 'ours', 'had', 'she', 'all', 'no', 'when', 'at', 'any', 'before', 'them', 'same', 'and', 'been', 'have', \n",
    "             'in', 'will', 'on', 'does', 'yourselves', 'then', 'that', 'because', 'what', 'over', 'why', 'so', 'can', 'did', \n",
    "             'not', 'now', 'under', 'he', 'you', 'herself', 'has', 'just', 'where', 'too', 'only', 'myself', 'which', 'those', \n",
    "             'i', 'after', 'few', 'whom', 't', 'being', 'if', 'theirs', 'my', 'against', 'a', 'by', 'doing', 'it', 'how', \n",
    "             'further', 'was', 'here', 'than', ''] \n",
    "\n",
    "def rem_stop(text):\n",
    "    text = tokenize(text)\n",
    "    text = [word for word in text if word not in stopword]\n",
    "    return text\n",
    "\n",
    "def preprocess(text):\n",
    "    text = punc_rem(text)\n",
    "    text = rem_stop(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['Processed_Text'] = train_data['text'].apply(lambda x: preprocess(x))\n",
    "validation_data['Processed_Text'] = validation_data['text'].apply(lambda x: preprocess(x))\n",
    "test_data['Processed_Text'] = test_data['text'].apply(lambda x: preprocess(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorizing sentences with Bag of Words\n",
    "\n",
    "Now that we have loaded in our data, we will need to vectorize our sentences - this is necessary to be able to numericalize our inputs before feeding them into our model. \n",
    "\n",
    "We will be using a Bag of Words approach to vectorize our sentences. This is a simple approach that counts the number of times each word appears in a sentence. \n",
    "\n",
    "The element at index $\\text{i}$ of the vector will be the number of times the $\\text{i}^{\\text{th}}$ word in our vocabulary appears in the sentence. So, for example, if our vocabulary is `[\"the\", \"cat\", \"sat\", \"on\", \"mat\"]`, and our sentence is `\"the cat sat on the mat\"`, then our vector will be `[2, 1, 1, 1, 1]`.\n",
    "\n",
    "We will now create a `BagOfWords` class to vectorize our sentences. This will involve creating\n",
    "\n",
    "1. A vocabulary from our corpus\n",
    "\n",
    "2. A mapping from words to indices in our vocabulary\n",
    "\n",
    "3. A function to vectorize a sentence in the fashion described above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15086\n"
     ]
    }
   ],
   "source": [
    "# Making the Vocabulary using the Train Data\n",
    "all_texts = train_data['Processed_Text']\n",
    "\n",
    "vocabulary = set()\n",
    "for row in all_texts:\n",
    "    for element in row:\n",
    "        vocabulary.add(element)\n",
    "        \n",
    "vocabulary = list(vocabulary)\n",
    "print(len(vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to make a vector from a string\n",
    "def vectorize(text,vocab):\n",
    "    words = text       \n",
    "    bag_vector = np.zeros(len(vocab),dtype=np.uint16)        \n",
    "    for w in words:\n",
    "        # A problem of out of vocabulary words occured when seeing new data. for classification purposes, we can simply ignore such words\n",
    "        if w in vocab:\n",
    "            ind = vocab.index(w)\n",
    "            bag_vector[ind] += 1 \n",
    "    return bag_vector\n",
    "\n",
    "# Function to vectorize a pandas data structure\n",
    "def vectorize_pd(text_list,vocab):\n",
    "    total_bags = []\n",
    "    for sentence in text_list:   \n",
    "        bag_vector = vectorize(sentence,vocab)\n",
    "        total_bags.append(np.array(bag_vector))     \n",
    "    return total_bags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a sanity check, we manually set the vocabulary of your `BagOfWords` object to the vocabulary of the example above, and check that the vectorization of the sentence is correct.\n",
    "\n",
    "Once we have implemented the `BagOfWords` class, we fit it to the training data, and vectorize the training, validation, and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 1 1 1 1]\n",
      "[0 1 1 0 1]\n"
     ]
    }
   ],
   "source": [
    "# Sanity check\n",
    "example_vocab = [\"the\", \"cat\", \"sat\", \"on\", \"mat\"]\n",
    "s = \"the cat sat on the mat\"\n",
    "print(vectorize(tokenize(s),example_vocab))\n",
    "print(vectorize(preprocess(s),example_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "bags = vectorize_pd(train_data['Processed_Text'],vocabulary)\n",
    "train_data['bow'] = bags\n",
    "\n",
    "bags2 = vectorize_pd(validation_data['Processed_Text'],vocabulary)\n",
    "validation_data['bow'] = bags2\n",
    "\n",
    "bags3 = vectorize_pd(test_data['Processed_Text'],vocabulary)\n",
    "test_data['bow'] = bags3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes\n",
    "\n",
    "### From Scratch\n",
    "\n",
    "Now that we have vectorized our sentences, we can implement our Naive Bayes model. Recall that the Naive Bayes model is based off of the Bayes Theorem:\n",
    "\n",
    "$$\n",
    "P(y \\mid x) = \\frac{P(x \\mid y)P(y)}{P(x)}\n",
    "$$\n",
    "\n",
    "What we really want is to find the class $c$ that maximizes $P(c \\mid x)$, so we can use the following equation:\n",
    "\n",
    "$$\n",
    "\\hat{c} = \\underset{c}{\\text{argmax}} \\ P(c \\mid x) = \\underset{c}{\\text{argmax}} \\ P(x \\mid c)P(c)\n",
    "$$\n",
    "\n",
    "We can then use the Naive Bayes assumption to simplify this:\n",
    "\n",
    "$$\n",
    "\\hat{c} = \\underset{c}{\\text{argmax}} \\ P(c \\mid x) = \\underset{c}{\\text{argmax}} \\ P(c) \\prod_{i=1}^{n} P(x_i \\mid c)\n",
    "$$\n",
    "\n",
    "Where $x_i$ is the $i^{\\text{th}}$ word in our sentence.\n",
    "\n",
    "All of these probabilities can be estimated from our training data. We can estimate $P(c)$ by counting the number of times each class appears in our training data, and dividing by the total number of training examples. We can estimate $P(x_i \\mid c)$ by counting the number of times the $i^{\\text{th}}$ word in our vocabulary appears in sentences of class $c$, and dividing by the total number of words in sentences of class $c$.\n",
    "\n",
    "It would help to apply logarithms to the above equation so that we translate the product into a sum, and avoid underflow errors. This will give us the following equation:\n",
    "\n",
    "$$\n",
    "\\hat{c} = \\underset{c}{\\text{argmax}} \\ \\log P(c) + \\sum_{i=1}^{n} \\log P(x_i \\mid c)\n",
    "$$\n",
    "\n",
    "We will now implement this algorithm. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function for counting frequencies in 2d array\n",
    "def counter(twod):\n",
    "    n = len(twod[0])\n",
    "    res = np.zeros((n,))\n",
    "    \n",
    "    for row in range(len(twod)):\n",
    "        for col in range(n):\n",
    "            if twod[row][col] > 0:\n",
    "                res[col] += 1\n",
    "            \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainNaiveBayes(D,C,vocab):\n",
    "    logprior = dict()\n",
    "    logliklihood = dict()\n",
    "    \n",
    "    # Repeating for all classes\n",
    "    for c in C:\n",
    "        N_doc = len(D)\n",
    "        N_c = (D['label'] == c).sum()\n",
    "        logprior[c] = math.log(N_c/N_doc)\n",
    "        \n",
    "        # Getting vocabulary of one class\n",
    "        bigdoc = D.loc[D['label'] == c]\n",
    "        class_vocab = set()\n",
    "        class_texts = bigdoc['Processed_Text']\n",
    "        for row in class_texts:\n",
    "            for element in row:\n",
    "                class_vocab.add(element)\n",
    "                     \n",
    "        # Getting frequencies\n",
    "        bigdoc = bigdoc['bow'].values.tolist()\n",
    "        counts = counter(bigdoc)\n",
    "        \n",
    "        for word in vocab:\n",
    "            index = vocab.index(word)\n",
    "            count_w = counts[index]        \n",
    "            count_not_w = len(vocab) - 1\n",
    "            count_not_w += counts.sum()\n",
    "            count_not_w -= count_w\n",
    "            frac = (count_w+1)/count_not_w\n",
    "            logliklihood[(word,c)] = math.log(frac)\n",
    "            \n",
    "    return logprior,logliklihood\n",
    "\n",
    "def testNaiveBayes(testdoc,logprior,logliklihood,C):\n",
    "    sums = dict()\n",
    "    for c in C:\n",
    "        sums[c] = logprior[c]\n",
    "        for word in testdoc:\n",
    "            if word in vocabulary:\n",
    "                sums[c] += logliklihood[(word,c)]\n",
    "            \n",
    "    return max(sums, key=sums.get)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now using the implementation to train a Naive Bayes model on the training data, we generate predictions for the Validation Set.\n",
    "\n",
    "We report the Accuracy, Precision, Recall, and F1 score of the model on the validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training model\n",
    "classes = [0,1,2,3,4,5]\n",
    "prior, likelihoods = trainNaiveBayes(train_data,classes,vocabulary)\n",
    "\n",
    "# Making predictions on test data\n",
    "test_features = (test_data['Processed_Text']).values.tolist()\n",
    "correct = (test_data['label']).values.tolist()\n",
    "prediction_list = []\n",
    "for f in test_features:\n",
    "    p = testNaiveBayes(f,prior,likelihoods,classes)\n",
    "    prediction_list.append(p)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is:  0.7865\n",
      "Precision is:  0.8747333462672048\n",
      "Recall is:  0.7865\n",
      "F1 is:  0.8133268336502549\n",
      "Confusuion Matrix is: \n",
      "[[541  15  26  64  49  17]\n",
      " [ 33 673  80  32  33  32]\n",
      " [  0   5  48   0   0   0]\n",
      " [  4   0   4 173   7   0]\n",
      " [  3   2   1   6 135  14]\n",
      " [  0   0   0   0   0   3]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy is: \", accuracy_score(prediction_list,correct))\n",
    "print(\"Precision is: \", precision_score(prediction_list,correct,average='weighted'))\n",
    "print(\"Recall is: \", recall_score(prediction_list,correct,average='weighted'))\n",
    "print(\"F1 is: \", f1_score(prediction_list,correct,average='weighted'))\n",
    "print(\"Confusuion Matrix is: \")\n",
    "print(confusion_matrix(prediction_list,correct))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using `sklearn`\n",
    "\n",
    "Now that we have implemented your own Naive Bayes model, we will use the `sklearn` library to train a Naive Bayes model on the same data. Alongside this, we will use their implementation of the Bag of Words model, the `CountVectorizer` class, to vectorize your sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making Bag of Words for train and test data\n",
    "corpus = train_data[\"text\"]\n",
    "vectorizer = CountVectorizer()\n",
    "sk_train_vectors = vectorizer.fit_transform(corpus)\n",
    "sk_covab = vectorizer.get_feature_names_out()\n",
    "\n",
    "sk_test_vectors = vectorizer.transform(test_data[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training model\n",
    "clf = MultinomialNB()\n",
    "clf.fit(sk_train_vectors, train_data['label'])\n",
    "\n",
    "# Making predictions on test data\n",
    "pred = clf.predict(sk_test_vectors)\n",
    "correct = (test_data['label']).values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is:  0.7655\n",
      "Precision is:  0.8783994550412492\n",
      "Recall is:  0.7655\n",
      "F1 is:  0.8007682644551712\n",
      "Confusuion Matrix is: \n",
      "[[546  16  27  66  59  21]\n",
      " [ 29 674  91  46  39  32]\n",
      " [  0   2  36   0   0   0]\n",
      " [  2   1   4 156   7   0]\n",
      " [  4   2   1   7 119  13]\n",
      " [  0   0   0   0   0   0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\khuze\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy is: \", accuracy_score(pred,correct))\n",
    "print(\"Precision is: \", precision_score(pred,correct,average='weighted'))\n",
    "print(\"Recall is: \", recall_score(pred,correct,average='weighted'))\n",
    "print(\"F1 is: \", f1_score(pred,correct,average='weighted'))\n",
    "print(\"Confusuion Matrix is: \")\n",
    "print(confusion_matrix(pred,correct))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "envx",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
